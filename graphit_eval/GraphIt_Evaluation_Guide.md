#GraphIt Code Generation and Performance Evaluation Guide

The following overview consists of a Step by Step Instructions explaining how to reproduce Figure 6 (PageRankDelta with different schedules) and Table 8 (GraphIt performance on our 2-socket machine) in the paper. We refer users to the [Getting Started Guide](https://github.com/GraphIt-DSL/graphit/blob/master/README.md ) for instructions to set up GraphIt. NOTE: the schedules we used here are almost certainly **NOT** the fastest schedules for your machine. **Please only use the instructions here as examples for writing and compiling different schedules, and tune schedules to best fit your machine's features, such as cache size, number of sockets, and number of cores.**


## Reproducing Figure 6 (PageRankDelta with different schedules)
Figure 6 in the paper shows the different C++ code generated by applying different schedules to PageRankDelta. We have build a script to generate the code for PageRankDelta with different schedules and make sure the generated C++ code compiles successfully.

**This script might run for 4-5 minutes as compiling GraphIt code currently is a bit slow. Please wait for a few minutes for the compilation process to finish.**

```
   #start from graphit root directory
  cd  graphit_eval/
  cd pagerankdelta_example
  python compile_pagerankdelta_fig6.py
```

The program should output the information on each schedule, print the generated C++ file to stdout, and save the generated file in .cpp files in the directory. The schedules we used are stored in `pagerankdelta_example/schedules`. We added a cache optimized schedule that was not included in the paper due to space constraints. This experiment demonstrates GraphIt's ability to compose together cache, direction, parallelization and data structure optimizations.

## Reproducing Table 8 for GraphIt
Table 7 in the paper shows the performance numbers of GraphIt and other frameworks on 6 applications. Here we provide a script that can produce GraphIt's performance for PageRank, PageRankDelta, Breadth-First Search, Single Source Shortest Paths, and Connected Components. Collaborative Filtering is not included in the script as we do not have the right to distribute the Netflix dataset, but we leave instructions for reviewers to compile collaborative filtering in case Netflix dataset is available. Reviewers can download the other frameworks from their github repositories to replicate the performance of the other frameworks.

### Running GraphIt generated programs

The following commands run the serial version of GraphIt on a small test graph (both the unweighted and weighted versions are in `graphit/graphit_eval/data/testGraph `) that is included in the repository. We have included the generated optimized C++ files for our dual-socket machine in the `graphit_eval/eval/table7/cpps` directory.

```
#start from graphit root directory
cd  graphit_eval/eval/table7

#first compile the generated cpp files
make

#run and benchmark the performance
python table7_graphit.py
```

The script first runs the benchmarks and then saves the outputs to the `graphit_eval/eval/table7/outputs/` directory. The benchmark script choose the binary based on the graph. Then a separate script parses the outputs to generate the final table of performance in the following form. The application and graph information are shown in the leftmost column, and the running times are shown in the second column in seconds.

```
{'graphit': {'testGraph': {'bfs': 3.3333333333333335e-07, 'pr': 1e-06, 'sssp': 5e-07, 'cc': 1e-06, 'prd': 3e-06}}}
bfs
testGraph, 3.33333333333e-07
sssp
testGraph, 5e-07
pr
testGraph, 1e-06
cc
testGraph, 1e-06
prd
testGraph, 3e-06
Done parsing the run outputs
```
These runs should complete very quickly.

The performance in the VM does not reflect the actual performance because the VM has a single core and has limited memory. This script shows the ability for users / reviewers to replicate the performance on some hardware. Optional instructions on replicating the performance in a physical machine are described in later sections.

### Running on additional graphs (optional)

We have provided a few slightly larger graphs for testing. In the folder we have socLive.sg (unweighted binary Live Journal graph), socLive.wsg (weighted binary Live Journal graph). Outside of the compressed file, we have road graph and Twitter graph in the `additional_graphit_graphs` directory. The VM has **insufficient memory** to run Live Journal and the additional graphs. We recommend running these graphs on a machine with at least 8 GB memory. Running the experiments on Twitter graph can potentially take a significant amount of time if your machine does not have a 100 GB memory and many cores. Running these other graphs with serial C++ implementations are very slow. Try to use the parallel implementations if possible (instructions given in later sections).

Below we first show the instructions for running the socLive (Live Journal) graph.

```
#copy the files to the data directories.
#The directory names have to be socLive, road-usad, twitter as we used hard-coded names in the scripts.

mkdir graphit/graphit_eval/eval/data/socLive
cp socLive.sg graphit/graphit_eval/eval/data/socLive
cp socLive.wsg graphit/graphit_eval/eval/data/socLive

#start from graphit root directory
cd  graphit_eval/eval/table7

#first compile the generated cpp files
make

#run and benchmark the performance
python table7_graphit.py --graph socLive

```

The road graph and Twitter graph need to be named as `road-usad` and `twitter`. We have included some instructions below.

```
#copy the files to the data directories.
#The directory names have to be socLive, road-usad, twitter as we used hard-coded names in the scripts.

mkdir graphit/graphit_eval/eval/data/road-usad
cp road-usad.sg graphit/graphit_eval/eval/data/road-usad
cp road-usad.wsg graphit/graphit_eval/eval/data/road-usad

mkdir graphit/graphit_eval/eval/data/twitter
cp twitter.sg graphit/graphit_eval/eval/data/twitter
cp twitter.wsg graphit/graphit_eval/eval/data/twitter

#start from graphit root directory
cd  graphit_eval/eval/table7

#first compile the generated cpp files
#see the section below for info on parallel builds
make

#run and benchmark the performance on both road and Twitter graphs
python table7_graphit.py --graph road-usad twitter

```


### Running parallel versions and replicating performance (optional)


Here we list the instructions for compiling the generated C++ files using icpc or gcc with Cilk and OpenMP. The user mostly need to define a few variables for the Makefile.

```
#start from graphit root directory
cd  graphit_eval/eval/table7

#compile with icpc if you installed the intel compiler
make ICPC_PAR=1

#compile with gcc with Cilk and OpenMP
make GCC_PAR=1

#run and benchmark the performance
python table7_graphit.py --graph socLive
```

As we mentioned earlier, the VM is not a good place to replicate the performance numbers we reported in the paper. To replicate the performance, you will need to 1) use the parallel versions of the generated C++ programs 2) run them on a machine with similar configurations as ours. We used Intel Xeon E5-2695 v3 CPUs with 12 cores
each for a total of 24 cores and 48 hyper-threads. The system has 128GB of DDR3-1600 memory
and 30 MB last level cache on each socket, and runs with Transparent Huge Pages (THP) enabled. The generated C++ files are also not optimized for single-socket or 4-socket machines.

### Generating, converting and testing graphs (optional)

GraphIt reuses [GAPBS input formats](https://github.com/sbeamer/gapbs). Specifically, we have tested with edge list file (.el), weighted edge list file (.wel), binary edge list (.sg), and weighted binary edge list (.wsg) formats. Users can use the converter in GAPBS (GAPBS/src/converter.cc) to convert other graph formats into the supported formats, or convert weighted and unweighted edge list files into their respective binary formats.

For the additional graphs, you can use the compiled binaries in the `graphit_eval/eval/table7/bin` directory to evaluate the performance. We normally use `numactl -i all` on graphs with 10M+ vertices. `graphit_eval/eval/table7/benchmark.py` has examples of commands used for running the generated binaries.

To use the script for additional graphs, follow the example of socLive on creating directories in `graphit/graphit_eval/eval/data/`. However, certain graphs have to be named in a certain way in order to use our provided script. For example, road graph and Twitter graph need to be named as `road-usad` and `twitter`. Please take a look at `graphit_eval/eval/table7/benchmark.py` for more details.


### Generating the C++ files from GraphIt programs (optional)

The algorithms we used for benchmarking, such as PageRank, PageRankDelta, BFS, Connected Components, Single Source Shortest Paths and Collaborative Filtering are in the **apps** directory.
These files include ONLY the algorithm and NO schedule. You need to use the appropriate schedules for the specific algorithm and input graph to get the best performance.

In the [arxiv paper](https://arxiv.org/abs/1805.00923) (Table 8), we described the schedules used for each algorithm on each graph on a dual socket system with Intel Xeon E5-2695 v3 CPUs with 12 cores
each for a total of 24 cores and 48 hyper-threads. The system has 128GB of DDR3-1600 memory
and 30 MB last level cache on each socket, and runs with Transparent Huge Pages (THP) enabled. The best schedule for a different machine can be different. You might need to try a few different set of schedules for the best performance.

In the schedules shown in Table 8, the keyword ’Program’ and the continuation symbol ’->’ are omitted. ’ca’ is the abbreviation for ’configApply’. Note that configApplyNumSSG uses an integer parameter (X) which is dependent on the graph size and the cache size of a system. For example, the complete schedule used for CC on Twitter graph is the following (X is tuned based on the cache size)

```
schedule:
    program->configApplyDirection("s1", "SparsePush-DensePull")->configApplyParallelization("s1", "dynamic-vertex-parallel")->configApplyDenseVertexSet("s1","bitvector", "src-vertexset", "DensePull");
    program->configApplyNumSSG("s1", "fixed-vertex-count",  X, "DensePull");
```

The **test/input** and **test/input\_with\_schedules** directories contain many examples of the algorithm and schedule files. Use them as references when writing your own schedule and generate C++ implementations.
